{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from models.create_dataset import create_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import hyperopt\n",
    "import pickle\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "                    , handlers=[logging.FileHandler('transformer_run.log'), logging.StreamHandler()])\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# create dataset\n",
    "target_col = 'DK1_price'\n",
    "df = create_dataset(target_col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Pivot hourly index out to columns so index is only date\n",
    "pivot_columns = [col for col in X.columns if not col.startswith('day_of_week')]\n",
    "X = X.pivot_table(index=X.index.date, columns=X.index.hour, values=pivot_columns)\n",
    "X.columns = [f'hour_{col}' for col in X.columns]\n",
    "X = X.dropna()\n",
    "# Some hours will only have 0 values, drop these columns (e.g. Solar)\n",
    "X = X.loc[:, (X != 0).any(axis=0)]\n",
    "# and some are 0 almost always, drop features with a MAD below threshold\n",
    "X = X.loc[:, X.sub(X.median(axis=0), axis=1).abs().median(axis=0) > 0.01]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "\n",
    "    def __init__(self, median=None, mad=None):\n",
    "        self.median = None\n",
    "        self.mad = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = pd.DataFrame(data)\n",
    "        self.median = data.median(axis=0).to_numpy().reshape(1, len(data.columns))\n",
    "        # calculate median absolute deviation\n",
    "        self.mad = data.sub(data.median(axis=0), axis=1).abs().median(axis=0).to_numpy().reshape(1, len(data.columns))\n",
    "        # print na in mad\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.median is None or self.mad is None:\n",
    "            raise ValueError('Fit scaler first')\n",
    "\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = pd.DataFrame(data)\n",
    "        X_transformed = data.sub(self.median, axis=1)\n",
    "        X_transformed = X_transformed.div(self.mad, axis=1)\n",
    "        X_transformed = np.arcsinh(X_transformed)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "\n",
    "        if self.median is None or self.mad is None:\n",
    "            raise ValueError('Fit scaler first')\n",
    "        # fix so this works for series and dataframe\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = pd.DataFrame(data)\n",
    "\n",
    "        X_inversed = np.sinh(data)\n",
    "        X_inversed = X_inversed.mul(self.mad, axis=1)\n",
    "        X_inversed = X_inversed.add(self.median, axis=1)\n",
    "        # make this work for series\n",
    "\n",
    "\n",
    "        return X_inversed\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#X = transform(X)\n",
    "X.index = pd.to_datetime(X.index)\n",
    "X['day_of_week'] = X.index.dayofweek\n",
    "\n",
    "# to dummies\n",
    "# day_of_week_0 column when day_of_week is 0, i.e. monday. 1 if monday, 0 otherwise\n",
    "X['day_of_week_0'] = X['day_of_week'].apply(lambda x: 1 if x == 0 else 0)\n",
    "X = pd.get_dummies(X, columns=['day_of_week'], drop_first=True) # last one should not be there, but we still use it?\n",
    "\n",
    "# Drop Nan rows ( should be only first because im running UTC and thus the first \"day\" doesn't have 24 hrs)\n",
    "\n",
    "# now y\n",
    "# make y to dataframe first, should alreadt be, but just to be sure\n",
    "y = pd.DataFrame(y)\n",
    "y = y.pivot_table(index=y.index.date, columns=y.index.hour, values=y.columns)\n",
    "# join multiindex columns to one, price with hour number\n",
    "y.index = pd.to_datetime(y.index)\n",
    "\n",
    "y = y.dropna()\n",
    "\n",
    "\n",
    "val_cutoff = pd.to_datetime('2020-07-01')\n",
    "test_cutoff = pd.to_datetime('2021-01-01')\n",
    "X_train = X.loc[X.index < val_cutoff]\n",
    "X_val = X.loc[(X.index >= val_cutoff) & (X.index < test_cutoff)]\n",
    "X_test = X.loc[X.index >= test_cutoff]\n",
    "y_train = y.loc[y.index < val_cutoff]\n",
    "y_val = y.loc[(y.index >= val_cutoff) & (y.index < test_cutoff)]\n",
    "y_test = y.loc[y.index >= test_cutoff]\n",
    "# test write 2+2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        #encoder_layers = [nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads) for _ in range(num_encoder_layers)]\n",
    "        #decoder_layers = [nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads) for _ in range(num_decoder_layers)]\n",
    "        #self.transformer_layers = nn.ModuleList(encoder_layers + decoder_layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, num_encoder_layers, num_decoder_layers, num_heads, hidden_dim, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(n_features, hidden_dim)\n",
    "        # Encoder decoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.TransformerEncoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, 24)  # Output size changed to 24\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # [seq_length, batch_size, hidden_dim]\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.TransformerEncoder(x)\n",
    "        # Decoder\n",
    "        decoder_output = self.TransformerDecoder(x, encoder_output)\n",
    "\n",
    "        decoder_output = decoder_output.permute(1, 0, 2)  # [batch_size, seq_length, hidden_dim]\n",
    "        x = self.fc(decoder_output)\n",
    "        x = x.mean(dim=1)  # Reduce the sequence length dimension to obtain [batch_size, 24]\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def create_sequences(X, sequence_length):\n",
    "    X_sequences = []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    X_sequences = torch.tensor(X_sequences).float()\n",
    "    # swap dim 1 and 2\n",
    "\n",
    "\n",
    "    return X_sequences\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def train_val_test_sequences(train_date_from, val_cutoff, test_cutoff, seq_length, batch_size):\n",
    "    # Scale data according to training data\n",
    "    # sequence for all data.\n",
    "    # output only index between data_date_from and data_date_to\n",
    "    # The dates are inclusive\n",
    "    func_time = time.time()\n",
    "    XScaler = Scaler()\n",
    "    X_local = X.copy()\n",
    "    y_local = y.copy()\n",
    "    X_train = X_local[(X_local.index >= train_date_from) & (X_local.index < val_cutoff)]\n",
    "    XScaler.fit(X_train.iloc[:, :-7])\n",
    "    X_scaled = XScaler.transform(X_local.iloc[:, :-7])\n",
    "    # add dummies\n",
    "    X_scaled = pd.concat([X_scaled, X_local.iloc[:, -7:]], axis=1)\n",
    "    yScaler = Scaler()\n",
    "    yScaler.fit(y_local[(y_local.index >= train_date_from) & (y_local.index < val_cutoff)])\n",
    "    y_scaled = yScaler.transform(y)\n",
    "    # create sequences for all data\n",
    "    X_seq = create_sequences(X_scaled.values, seq_length)\n",
    "    # get index of data to use\n",
    "\n",
    "    # we need to drop sequence length from the index as we have lost that many rows\n",
    "    X_local = X_local.iloc[seq_length:]\n",
    "    # if test is iterable do smth\n",
    "    val_idx = np.where(X_local.index == val_cutoff)[0][0]\n",
    "    test_idx = np.where(X_local.index == test_cutoff)[0][0]\n",
    "\n",
    "    # split X_seq and y_seq tensors into train, val and test sets\n",
    "    X_train_seq = X_seq[:val_idx]\n",
    "    y_train_seq = y_scaled.iloc[:val_idx] # slice is not including end, works like range\n",
    "    X_val_seq = X_seq[val_idx:test_idx]\n",
    "    y_val_seq = y_scaled.iloc[val_idx:test_idx]\n",
    "    X_test_seq = X_seq[test_idx] # gets the index of the test date\n",
    "    y_test_seq = y_scaled.iloc[test_idx]\n",
    "\n",
    "    # y to tensor\n",
    "    y_train_seq = torch.tensor(y_train_seq.values).float()\n",
    "    y_val_seq = torch.tensor(y_val_seq.values).float()\n",
    "    y_test_seq = torch.tensor(y_test_seq.values).float()\n",
    "\n",
    "    # add dim for test as this is only one dim, then we now both have row\n",
    "    X_test_seq = X_test_seq.unsqueeze(0)\n",
    "    y_test_seq = y_test_seq.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_seq, y_test_seq), batch_size=1, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, XScaler, yScaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "sequence_length = 24\n",
    "n_feat = X.shape[1]\n",
    "batch_size = 16\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "num_heads = 4\n",
    "hidden_dim = num_heads * 4\n",
    "dropout = 0.1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "model = TransformerModel(n_features=X.shape[1], seq_length=sequence_length, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, num_heads=num_heads, hidden_dim=hidden_dim, drop_out=dropout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "train_date_from = X.index[0]\n",
    "val_cutoff = pd.to_datetime('2020-07-01')\n",
    "test_cutoff = pd.to_datetime('2021-01-01')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, x_scale, y_scale = train_val_test_sequences(train_date_from, val_cutoff, test_cutoff, sequence_length, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 24, 802])\n",
      "torch.Size([16, 24])\n",
      "torch.Size([16, 24])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape)  # [batch_size, seq_length, n_features]\n",
    "    print(targets.shape)  # [batch_size, 24]\n",
    "    print(model(inputs).shape)\n",
    "\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tree structured parzen estimator\n",
    "\n",
    "optimize_hyperparameters = True\n",
    "\n",
    "batch_size = [2, 4, 8, 16, 32, 64, 128]\n",
    "num_encoder_layers = [i for i in range(1, 11)]\n",
    "num_decoder_layers = [i for i in range(1, 11)]\n",
    "seq_length = [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "num_heads = [i for i in range(2, 21, 2)]\n",
    "hidden_dim_multiplier = [i for i in range(2, 65, 2)]\n",
    "if optimize_hyperparameters:\n",
    "    from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "    epochs = 50\n",
    "    criterion = nn.MSELoss() # maybe not this one, but for now\n",
    "\n",
    "    # Define the search space\n",
    "    space = {\n",
    "        'weight_decay': hp.loguniform('weight_decay', -10, -1),\n",
    "        'num_encoder_layers': hp.choice('num_layers', num_encoder_layers),\n",
    "        'num_decoder_layers': hp.choice('num_layers', num_decoder_layers),\n",
    "        'hidden_size_dim_multiplier': hp.choice('hidden_size', hidden_dim_multiplier),\n",
    "        'num_heads': hp.choice('num_heads', num_heads),\n",
    "        'sequence_length': hp.choice('sequence_length', seq_length), # 1-24 hours\n",
    "        'batch_size': hp.choice('batch_size', batch_size),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -8, -1),\n",
    "        'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
    "    }\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(params, input_dim=X_train.shape[1], output_dim=24):\n",
    "        # Train and evaluate your model with the given hyperparameters\n",
    "        # Return the validation accuracy or other metric you want to optimize\n",
    "\n",
    "        params['batch_size'] = int(params['batch_size'])\n",
    "        params['hidden_size_dim_multiplier'] = int(params['hidden_size_dim_multiplier'])\n",
    "        params['sequence_length'] = int(params['sequence_length'])\n",
    "        params['num_heads'] = int(params['num_heads'])\n",
    "        params['num_encoder_layers'] = int(params['num_encoder_layers'])\n",
    "        params['num_decoder_layers'] = int(params['num_decoder_layers'])\n",
    "        hidden_dim = params['hidden_size_dim_multiplier'] * params['num_heads']\n",
    "        if params['num_layers'] == 1:  # if only one layer, no dropout as this occurs between layers\n",
    "            params['dropout_rate'] = 0\n",
    "\n",
    "        model = TransformerModel(n_features=input_dim, num_encoder_layers=params['num_encoder_layers'], num_decoder_layers=params['num_decoder_layers'], num_heads=params['num_heads'], hidden_dim=hidden_dim, drop_out=params['dropout_rate'])\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "        # fit scaler first, then transform: Fitting on training data then transforming on training and validation data\n",
    "        # sequence length after start date in X index\n",
    "        # get index value of 'sequence_length' days after start date\n",
    "        train_date_from_hyper = X_train.index[params['sequence_length']]\n",
    "        # get first index value of validation data and test data\n",
    "        val_date_from_hyper = X_val.index[0]\n",
    "        test_date_from_hyper = X_test.index[0]\n",
    "\n",
    "        train_loader, val_loader, test_loader, XScaler, yScaler = train_val_test_sequences(train_date_from=train_date_from_hyper,\n",
    "                                                                                           val_cutoff=val_date_from_hyper,\n",
    "                                                                                           test_cutoff=test_date_from_hyper,\n",
    "                                                                                           seq_length=params['sequence_length'],\n",
    "                                                                                           batch_size=params['batch_size'])\n",
    "\n",
    "\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            model.train()\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                # transfer to GPU\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "                # transfer to GPU\n",
    "\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            train_losses.append(train_loss/len(train_loader))\n",
    "            val_losses.append(val_loss/len(val_loader))\n",
    "            # early stopping\n",
    "            if epoch % 5 == 0:\n",
    "\n",
    "                # if val hasn't decreased for 5 epochs, stop\n",
    "                if min(val_losses[-3:]) > best_val_loss:\n",
    "                    break\n",
    "                best_val_loss = min(val_losses[-3:])\n",
    "\n",
    "        #accuracy = val_losses[-1]\n",
    "        # min of last 3 val losses\n",
    "        accuracy = min(val_losses[-3:])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    # Define the TPE algorithm\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    # Define the number of iterations\n",
    "    max_evals = 1000\n",
    "\n",
    "    # Initialize the trials object\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the TPE algorithm to optimize the hyperparameters\n",
    "    best_params = fmin(objective, space, algo=tpe_algorithm, max_evals=max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    # save best parameters to pkl\n",
    "    param_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\Transformer\\best_params.pkl'\n",
    "    pickle.dump(best_params, open(param_path, 'wb'))\n",
    "    trials_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\Transformer\\trials.pkl'\n",
    "    pickle.dump(trials, open(trials_path, 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load best hyper parameters\n",
    "param_path = os.path.join('.', 'best_params.pkl')\n",
    "best_params = pickle.load(open(param_path, 'rb'))\n",
    "\n",
    "# get another param to see if it works\n",
    "logger.info(f'Training model, with best hyperparameters {best_params}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_train_model(model, date_to_forecast, train_loader, val_loader, batch_size, epochs):\n",
    "    # setup model\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # transfer to GPU\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            assert not torch.isnan(inputs).any()\n",
    "            assert not torch.isnan(labels).any()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            # transfer to GPU\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "        # early stopping, every 5th epoch check if val loss has decreased\n",
    "        if epoch % 5 == 0:\n",
    "\n",
    "            # if val hasn't decreased for 5 epochs, stop\n",
    "            if val_losses[-1] > best_val_loss:\n",
    "                print(f'Terminated epoch {epoch + 1}/{epochs} early: train loss: {round(train_losses[-1], 2)}, val loss: {round(val_losses[-1], 2)}')\n",
    "                break\n",
    "            best_val_loss = val_losses[-1]\n",
    "\n",
    "        logger.info(f'Epoch {epoch + 1}/{epochs} complete, train loss: {train_losses[-1]}, val loss: {val_losses[-1]}')\n",
    "\n",
    "    #accuracy = val_losses[-1]\n",
    "    # min of last 3 val losses\n",
    "    # save model\n",
    "    model_path = os.path.join(os.getcwd(), 'models', f'{date_to_forecast.strftime(\"%Y-%m-%d\")}_transformer_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # save train and val losses\n",
    "    losses_path = os.path.join(os.getcwd(), 'losses', f'{date_to_forecast.strftime(\"%Y-%m-%d\")}_transformer_losses.pkl')\n",
    "    pickle.dump([train_losses, val_losses], open(losses_path, 'wb'))\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we want to go throught each day, train on all previous data except 1 week, then predict 1 day, then move on to next day\n",
    "# we want to save the predictions for each day, and then evaluate the model on the whole dataset\n",
    "\n",
    "# setup model\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 24\n",
    "\n",
    "epochs = 50\n",
    "##################################################\n",
    "best_params['batch_size'] = int(best_params['batch_size'])\n",
    "best_params['sequence_length'] = int(best_params['sequence_length'])\n",
    "best_params['hidden_size_dim_multiplier'] = int(best_params['hidden_size_dim_multiplier'])\n",
    "best_params['num_encoder_layers'] = int(best_params['num_encoder_layers'])\n",
    "best_params['num_decoder_layers'] = int(best_params['num_decoder_layers'])\n",
    "best_params['num_heads'] = int(best_params['num_heads'])\n",
    "best_params['hidden_dim'] = int(best_params['n_heads'] * best_params['hidden_size_dim_multiplier'])\n",
    "if best_params['num_encoder_layers'] == 1 or best_params['num_decoder_layers'] == 1:  # if only one layer, no dropout\n",
    "    best_params['dropout_rate'] = 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########### SETUP MODEL AND OPTIMIZER WITH BEST HYPERPARAMETERS #############\n",
    "\n",
    "\n",
    "## Train model initially on training data\n",
    "train_loader_init, val_loader_init, _, _, _ = train_val_test_sequences(train_date_from=X.index[0],\n",
    "                                                                                      val_cutoff=val_cutoff,\n",
    "                                                                                      test_cutoff=test_cutoff,\n",
    "                                                                                      seq_length=best_params['sequence_length'],\n",
    "                                                                                      batch_size=best_params['batch_size'])\n",
    "\n",
    "model = LSTM(input_size=input_dim, hidden_size=best_params['hidden_size'], num_layers=best_params['num_layers'], output_size=output_dim, dropout=best_params['dropout_rate']).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model, train_losses_initial, val_losses_initial = build_train_model(model=model,\n",
    "                                                                    date_to_forecast=test_cutoff,\n",
    "                                                                    train_loader=train_loader_init,\n",
    "                                                                    val_loader=val_loader_init,\n",
    "                                                                    batch_size=best_params['batch_size'],\n",
    "                                                                    epochs=epochs)\n",
    "#save losses\n",
    "losses_path = os.path.join(os.getcwd(), 'losses', f'transformer_initial_train_data_losses.pkl')\n",
    "pickle.dump([train_losses_initial, val_losses_initial], open(losses_path, 'wb'))\n",
    "\n",
    "predictions_path = os.path.join(os.getcwd(), 'predictions', f'transformer_predictions.pkl')\n",
    "predictions = []\n",
    "\n",
    "\n",
    "\n",
    "calibration_window = pd.Timedelta(days=2 * 365)\n",
    "\n",
    "# only keep last month of X_test - crashed during last part\n",
    "\n",
    "\n",
    "# load newest model\n",
    "# load_model = True\n",
    "# if load_model:\n",
    "#     model_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\LSTM\\models\\2022-11-30_lstm_model.pth'\n",
    "#     model = LSTM(input_size=input_dim, hidden_size=best_params['hidden_size'], num_layers=best_params['num_layers'], output_size=output_dim, dropout=best_params['dropout_rate']).to(device)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "start_time = time.time()\n",
    "for i, date in enumerate(X_test.index):\n",
    "    train_date_from = date - calibration_window\n",
    "    val_cutoff = date - pd.Timedelta(days=7)\n",
    "    test_cutoff = date\n",
    "    train_loader_date, val_loader_date, test_loader, X_scaler_date, y_scaler_date = train_val_test_sequences(train_date_from,\n",
    "                                                                                                             val_cutoff,\n",
    "                                                                                                             test_cutoff,\n",
    "                                                                                                             best_params['sequence_length'],\n",
    "                                                                                                             batch_size = best_params['batch_size'])\n",
    "    # train model\n",
    "    model, train_losses, val_losses = build_train_model(model=model,\n",
    "                                                        date_to_forecast=date,\n",
    "                                                        train_loader=train_loader_date,\n",
    "                                                        val_loader=val_loader_date,\n",
    "                                                        batch_size=best_params['batch_size'],\n",
    "                                                        epochs=epochs)\n",
    "\n",
    "    model = model.eval()\n",
    "    for input, target in test_loader:\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        pred = model(input).detach().cpu().numpy()\n",
    "\n",
    "    # make prediction into dataframe 1 row, y columns and index of date\n",
    "    y_pred = pd.DataFrame(pred, index=[date])\n",
    "    y_pred = y_scaler_date.inverse_transform(y_pred)\n",
    "    y_true = y_test[y_test.index == date]\n",
    "    predictions.append(y_pred)\n",
    "    # expectd runtime\n",
    "    elapsed_time = time.time() - start_time\n",
    "    expected_time = elapsed_time / (i + 1) * len(X_test.index)\n",
    "    print(f'Date: {date} Expected time remaining: {(expected_time - elapsed_time) / 3600:.2f} hours, MAE: {mean_absolute_error(y_pred, y_true):.2f}')\n",
    "\n",
    "    # every 3 months save predictions\n",
    "    if date.month % 3 == 0 and date.day == 30:\n",
    "        # save predictions\n",
    "        predictions_path = os.path.join(os.getcwd(), 'predictions', f'{date.strftime(\"%Y-%m-%d\")}_transformer_predictions_1.pkl')\n",
    "        pickle.dump(pd.concat(predictions, axis=0), open(predictions_path, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "# # read all previous predictions\n",
    "# prediction_files = ['2021-03-01_lstm_predictions.pkl',\n",
    "#                     '2021-06-01_lstm_predictions.pkl',\n",
    "#                     '2021-09-01_lstm_predictions.pkl',\n",
    "#                     '2021-12-01_lstm_predictions.pkl',\n",
    "#                     '2022-03-01_lstm_predictions.pkl',\n",
    "#                     '2022-06-01_lstm_predictions.pkl',\n",
    "#                     '2022-09-01_lstm_predictions.pkl',\n",
    "#                     '2022-12-01_lstm_predictions.pkl']\n",
    "#\n",
    "# for file in os.listdir(os.path.join(os.getcwd(), 'predictions')):\n",
    "#     if file in prediction_files:\n",
    "#         predictions.append(pickle.load(open(os.path.join(os.getcwd(), 'predictions', file), 'rb')))\n",
    "# # concat predictions\n",
    "\n",
    "transformer_all_preds_path = os.path.join(os.getcwd(), 'predictions', f'transformer_preds_all.pkl')\n",
    "predictions = pd.concat(predictions, axis=0)\n",
    "pickle.dump(predictions, open(transformer_all_preds_path, 'wb'))\n",
    "\n",
    "\n",
    "# save actual values\n",
    "actuals_path = os.path.join(os.getcwd(), 'predictions', f'transformer_actuals_all.pkl')\n",
    "pickle.dump(y_test, open(actuals_path, 'wb'))\n",
    "\n",
    "# read predictions\n",
    "\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "os.chdir('results_app')\n",
    "predictions_path = os.path.join(os.getcwd(), f'transformer_preds_all.pkl')\n",
    "pickle.dump(predictions, open(predictions_path, 'wb'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
