{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.create_dataset import create_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lasso model\n",
    "from epftoolbox.models import LEAR\n",
    "from sklearn.linear_model import Lasso\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO_Solar', 'NO_Solar_lag_1', 'NO_Solar_lag_2', 'NO_Solar_lag_3', 'NO_Solar_lag_7']\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "target_col = 'DK1_price'\n",
    "df = create_dataset(target_col)\n",
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Pivot hourly index out to columns so index is only date\n",
    "pivot_columns = [col for col in X.columns if not col.startswith('day_of_week')]\n",
    "X = X.pivot_table(index=X.index.date, columns=X.index.hour, values=X.columns)\n",
    "X.columns = [f'hour_{col}' for col in X.columns]\n",
    "# Drop Nan rows\n",
    "X = X.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now y\n",
    "y = pd.DataFrame({target_col: y.DK1_price}, index=y.index)\n",
    "y = y.pivot_table(index=y.index.date, columns=y.index.hour, values=y.columns)\n",
    "y.columns = [f'hour_{col}' for col in y.columns]\n",
    "y = y.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DNN4(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_neurons, dropout_rate):\n",
    "        # assure lenght of num_neurons is 3\n",
    "        assert len(num_neurons) == 3\n",
    "\n",
    "        super(DNN4, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        self.fc1 = nn.Linear(self.input_dim, num_neurons[0])\n",
    "        self.fc2 = nn.Linear(num_neurons[0], num_neurons[1])\n",
    "        self.fc3 = nn.Linear(num_neurons[1], num_neurons[2])\n",
    "        self.fc4 = nn.Linear(num_neurons[2], self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make dataloader\n",
    "# and small test to see if it works\n",
    "train_loader = DataLoader(dataset=list(zip(X_train.values, y_train.values)), batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tree structured parzen estimator\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "criterion = nn.MSELoss() # maybe not this one, but for now\n",
    "epochs = 20\n",
    "# Define the search space\n",
    "space = {\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128, 256]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "    'num_neurons_1': hp.quniform('num_neurons', 32, 512, 32),\n",
    "    'num_neurons_2': hp.quniform('num_neurons', 32, 512, 32),\n",
    "    'num_neurons_3': hp.quniform('num_neurons', 32, 512, 32),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.5)\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, input_dim=X_train.shape[1], output_dim=24, epochs=epochs):\n",
    "    # Train and evaluate your model with the given hyperparameters\n",
    "    # Return the validation accuracy or other metric you want to optimize\n",
    "    model = DNN4(input_dim, output_dim, [params['num_neurons_1'], params['num_neurons_2'], params['num_neurons_3']], params['dropout_rate'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    # __loader__\n",
    "    train_loader = DataLoader(dataset=list(zip(X_train.values, y_train.values)), batch_size=params['batch_size'], shuffle=False)\n",
    "    val_loader = DataLoader(dataset=list(zip(X_val.values, y_val.values)), batch_size=params['batch_size'], shuffle=False)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "\n",
    "    accuracy = val_losses[-1]\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Define the TPE algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Define the number of iterations\n",
    "max_evals = 100\n",
    "\n",
    "# Initialize the trials object\n",
    "trials = Trials()\n",
    "\n",
    "# Run the TPE algorithm to optimize the hyperparameters\n",
    "best_params = fmin(objective, space, algo=tpe_algorithm, max_evals=max_evals, trials=trials)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Linear NN\n",
    "layers = 4\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 24\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
