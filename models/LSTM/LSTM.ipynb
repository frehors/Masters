{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.create_dataset import create_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import tensor dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import MAE\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "                    , handlers=[logging.FileHandler('LSTM_epf.log'), logging.StreamHandler()])\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO_Solar', 'NO_Solar_lag_1', 'NO_Solar_lag_2', 'NO_Solar_lag_3', 'NO_Solar_lag_7']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "target_col = 'DK1_price'\n",
    "df = create_dataset(target_col=target_col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# split into X and y\n",
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Pivot hourly index out to columns so index is only date\n",
    "pivot_columns = [col for col in X.columns if not col.startswith('day_of_week')]\n",
    "X = X.pivot_table(index=X.index.date, columns=X.index.hour, values=pivot_columns)\n",
    "X = X.dropna()\n",
    "# Some hours will only have 0 values, drop these columns (e.g. Solar)\n",
    "X = X.loc[:, (X != 0).any(axis=0)]\n",
    "# and some are 0 almost always, drop features with a MAD below threshold\n",
    "X = X.loc[:, X.sub(X.median(axis=0), axis=1).abs().median(axis=0) > 0.01]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# now y\n",
    "# make y to dataframe first\n",
    "y = y.rename('Price')\n",
    "y = pd.DataFrame(y)\n",
    "y = y.pivot_table(index=y.index.date, columns=y.index.hour, values=y.columns)\n",
    "# join multiindex columns to one, price with hour number\n",
    "y = y.dropna()\n",
    "# rename to 'Price' for some reason for this toolbox to work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:5: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  X_train = X.loc[(X.index >= start_cutoff) & (X.index < val_cutoff)] ########## temporary\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:6: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  X_train = X.loc[X.index < val_cutoff]\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:7: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  X_val = X.loc[(X.index >= val_cutoff) & (X.index < test_cutoff)]\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:8: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  X_test = X.loc[X.index >= test_cutoff]\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:9: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  y_train = y.loc[(y.index < val_cutoff) & (y.index >= start_cutoff)] ########### temporary\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  y_train = y.loc[y.index < val_cutoff]\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:11: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  y_val = y.loc[(y.index >= val_cutoff) & (y.index < test_cutoff)]\n",
      "C:\\Users\\frede\\AppData\\Local\\Temp\\ipykernel_26468\\1260850503.py:12: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  y_test = y.loc[y.index >= test_cutoff]\n"
     ]
    }
   ],
   "source": [
    "##### temporary start cut off as well to see if it works\n",
    "#start_cutoff = pd.to_datetime('2019-01-01 00:00')\n",
    "val_cutoff = pd.to_datetime('2020-07-01')\n",
    "test_cutoff = pd.to_datetime('2021-01-01')\n",
    "#X_train = X.loc[(X.index >= start_cutoff) & (X.index < val_cutoff)] ########## temporary\n",
    "X_train = X.loc[X.index < val_cutoff]\n",
    "X_val = X.loc[(X.index >= val_cutoff) & (X.index < test_cutoff)]\n",
    "X_test = X.loc[X.index >= test_cutoff]\n",
    "#y_train = y.loc[(y.index < val_cutoff) & (y.index >= start_cutoff)] ########### temporary\n",
    "y_train = y.loc[y.index < val_cutoff]\n",
    "y_val = y.loc[(y.index >= val_cutoff) & (y.index < test_cutoff)]\n",
    "y_test = y.loc[y.index >= test_cutoff]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class Scaler():\n",
    "\n",
    "    def __init__(self, median=None, mad=None):\n",
    "        self.median = None\n",
    "        self.mad = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = pd.DataFrame(data)\n",
    "        self.median = data.median(axis=0).to_numpy()\n",
    "        # calculate median absolute deviation\n",
    "        self.mad = data.sub(data.median(axis=0), axis=1).abs().median(axis=0).to_numpy()\n",
    "        # print na in mad\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.median is None or self.mad is None:\n",
    "            raise ValueError('Fit scaler first')\n",
    "\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = pd.DataFrame(data)\n",
    "        X_transformed = data.sub(self.median, axis=1)\n",
    "        X_transformed = X_transformed.div(self.mad, axis=1)\n",
    "        X_transformed = np.arcsinh(X_transformed)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "\n",
    "        if self.median is None or self.mad is None:\n",
    "            raise ValueError('Fit scaler first')\n",
    "        # fix so this works for series and dataframe\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = pd.DataFrame(data)\n",
    "\n",
    "        X_inversed = np.sinh(data)\n",
    "        X_inversed = X_inversed.mul(self.mad, axis=1)\n",
    "        X_inversed = X_inversed.add(self.median, axis=1)\n",
    "        # make this work for series\n",
    "\n",
    "\n",
    "        return X_inversed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_().to(device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i+sequence_length])\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "    X_sequences = torch.tensor(X_sequences).float()\n",
    "    y_sequences = torch.tensor(y_sequences).float()\n",
    "    return X_sequences, y_sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:49:51<00:00, 38.99s/trial, best loss: 1.683862618181143]  \n",
      "Best hyperparameters: {'batch_size': 0, 'dropout_rate': 0.07581341030688578, 'hidden_size': 320.0, 'learning_rate': 0.004557564896692678, 'num_layers': 4, 'sequence_length': 4, 'weight_decay': 0.004622173580033262}\n"
     ]
    }
   ],
   "source": [
    "# Tree structured parzen estimator\n",
    "\n",
    "optimize_hyperparameters = True\n",
    "\n",
    "batch_size = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "num_layers = [1, 2, 3, 4, 5]\n",
    "seq_length = [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "if optimize_hyperparameters:\n",
    "    from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "    epochs = 50\n",
    "    criterion = nn.MSELoss() # maybe not this one, but for now\n",
    "\n",
    "    # Define the search space\n",
    "    space = {\n",
    "        'weight_decay': hp.loguniform('weight_decay', -10, -1),\n",
    "        'num_layers': hp.choice('num_layers', num_layers),\n",
    "        'sequence_length': hp.choice('sequence_length', seq_length), # 1-24 hours\n",
    "        'batch_size': hp.choice('batch_size', batch_size),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -8, -1),\n",
    "        'hidden_size': hp.quniform('hidden_size', 32, 512, 32),\n",
    "        'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
    "    }\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(params, input_dim=X_train.shape[1], output_dim=24):\n",
    "        # Train and evaluate your model with the given hyperparameters\n",
    "        # Return the validation accuracy or other metric you want to optimize\n",
    "        #print(f'num_layers: {params[\"num_layers\"]}, batch_size:{params[\"batch_size\"]}, seq_len{params[\"sequence_length\"]}')\n",
    "        #params['num_layers'] = num_layers[params['num_layers']]\n",
    "        params['batch_size'] = int(params['batch_size'])\n",
    "        params['hidden_size'] = int(params['hidden_size'])\n",
    "        params['num_layers'] = int(params['num_layers'])\n",
    "        params['sequence_length'] = int(params['sequence_length'])\n",
    "        if params['num_layers'] == 1: # if only one layer, no dropout\n",
    "            params['dropout_rate'] = 0\n",
    "\n",
    "        #model = LSTM(input_dim, output_dim, , params['dropout_rate']).to(device)\n",
    "        model = LSTM(input_size=input_dim, hidden_size=params['hidden_size'], num_layers=params['num_layers'], output_size=output_dim, dropout=params['dropout_rate']).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        # print layers\n",
    "        # __loader__\n",
    "        # fit scaler first, then transform: Fitting on training data then transforming on training and validation data\n",
    "\n",
    "        XScaler = Scaler()\n",
    "        # fit scaler without last 7 features\n",
    "        XScaler.fit(X_train.iloc[:, :-7])\n",
    "\n",
    "\n",
    "        X_train_scaled = XScaler.transform(X_train.iloc[:, :-7])\n",
    "        # add dummies\n",
    "        X_train_scaled = pd.concat([X_train_scaled, X_train.iloc[:, -7:]], axis=1)\n",
    "        X_val_scaled = XScaler.transform(X_val.iloc[:, :-7])\n",
    "        X_val_scaled = pd.concat([X_val_scaled, X_val.iloc[:, -7:]], axis=1)\n",
    "\n",
    "        yScaler = Scaler()\n",
    "        yScaler.fit(y_train)\n",
    "        y_train_scaled = yScaler.transform(y_train)\n",
    "        y_val_scaled = yScaler.transform(y_val)\n",
    "        #train_dataset = Dataset()\n",
    "        #train_dataset.load_data(X_train_scaled, y_train_scaled)\n",
    "        #train_loader = DataLoader(dataset=list(zip(X_train_scaled.values, y_train_scaled.values)), batch_size=params['batch_size'], shuffle=False)\n",
    "        # make torch dataset\n",
    "        X_train_sequences, y_train_sequences = create_sequences(X_train_scaled.values, y_train_scaled.values, params['sequence_length'])\n",
    "        X_val_sequences, y_val_sequences = create_sequences(X_val_scaled.values, y_val_scaled.values, params['sequence_length'])\n",
    "        train_dataset = TensorDataset(X_train_sequences, y_train_sequences)\n",
    "        val_dataset = TensorDataset(X_val_sequences, y_val_sequences)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            model.train()\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                # transfer to GPU\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "                # transfer to GPU\n",
    "\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            train_losses.append(train_loss/len(train_loader))\n",
    "            val_losses.append(val_loss/len(val_loader))\n",
    "            # early stopping\n",
    "            if epoch % 5 == 0:\n",
    "\n",
    "                # if val hasn't decreased for 5 epochs, stop\n",
    "                if min(val_losses[-3:]) > best_val_loss:\n",
    "                    break\n",
    "                best_val_loss = min(val_losses[-3:])\n",
    "\n",
    "        #accuracy = val_losses[-1]\n",
    "        # min of last 3 val losses\n",
    "        accuracy = min(val_losses[-3:])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    # Define the TPE algorithm\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    # Define the number of iterations\n",
    "    max_evals = 1000\n",
    "\n",
    "    # Initialize the trials object\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the TPE algorithm to optimize the hyperparameters\n",
    "    best_params = fmin(objective, space, algo=tpe_algorithm, max_evals=max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    # save best parameters to json\n",
    "    param_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\LSTM\\best_params.pkl'\n",
    "    pickle.dump(best_params, open(param_path, 'wb'))\n",
    "    trials_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\LSTM\\trials.pkl'\n",
    "    pickle.dump(trials, open(trials_path, 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "best_params['batch_size'] = batch_size[best_params['batch_size']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "best_params['num_layers'] = num_layers[best_params['num_layers']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "best_params['sequence_length'] = seq_length[best_params['sequence_length']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length[best_params['sequence_length']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'batch_size': 2, 'dropout_rate': 0.07581341030688578, 'hidden_size': 320.0, 'learning_rate': 0.004557564896692678, 'num_layers': 5, 'sequence_length': 6, 'weight_decay': 0.004622173580033262}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters:\", best_params)\n",
    "# save best parameters to json\n",
    "param_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\LSTM\\best_params.pkl'\n",
    "pickle.dump(best_params, open(param_path, 'wb'))\n",
    "trials_path = r'C:\\Users\\frede\\PycharmProjects\\Masters\\models\\LSTM\\trials.pkl'\n",
    "pickle.dump(trials, open(trials_path, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "<property at 0x17f01c29da0>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trials.best_trial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "1.683862618181143"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial['result']['loss']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
